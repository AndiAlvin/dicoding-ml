# -*- coding: utf-8 -*-
"""nlp-dicoding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/101HjghaPPGU1TVJuj0jR1D-H-tyf8j8b
"""

import pandas as pd
import numpy as np

df = pd.read_csv('alldata_1_for_kaggle.csv', encoding='latin-1')

df.head()

df['0'].unique()

df.columns = ['index', 'disease', 'text']

import string

def remove_punctuation(text):
  no_punc = [word for word in text if word not in string.punctuation]
  words_no_punc = ''.join(no_punc)
  return words_no_punc

df['text_no_punc'] = df['text'].apply(lambda x: remove_punctuation(x))
df.head()

import re

def tokenize(text):
  split = re.split('\W+', text)
  return split

df['text_no_punc_split'] = df['text_no_punc'].apply(lambda x: tokenize(x.lower()))
df.head()

import nltk

#nltk.download('stopwords')

stopword = nltk.corpus.stopwords.words('english')

def remove_stopwords(text):
  text = [word for word in text if word not in stopword]
  return text

df['text_no_punc_split_no_stw'] = df['text_no_punc_split'].apply(lambda x: remove_stopwords(x))
df.head()

#nltk.download('wordnet')
#nltk.download('omw-1.4')

lematizer = nltk.stem.WordNetLemmatizer()

def word_lemmatizer(text):
  lemma_text = [lematizer.lemmatize(word) for word in text]
  lemma_text_joined = ' '.join(lemma_text)
  return lemma_text_joined

df['text_no_punc_split_no_stw_lemm'] = df['text_no_punc_split_no_stw'].apply(lambda title: word_lemmatizer(title))
df.head()

df.disease.isna().sum()

disease = pd.get_dummies(df.disease)
df = pd.concat([df, disease], axis=1)
df = df.drop(columns=['index', 'disease', 'text', 'text_no_punc', 'text_no_punc_split', 'text_no_punc_split_no_stw'])
df.head()

text = df['text_no_punc_split_no_stw_lemm'].values
label = df[['Colon_Cancer', 'Lung_Cancer', 'Thyroid_Cancer']].values

"""### Split data"""

from sklearn.model_selection import train_test_split
text_train, text_test, label_train, label_test = train_test_split(text, label, test_size=0.2)

"""## Model"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
 
tokenizer = Tokenizer(num_words=5000, oov_token='-')
tokenizer.fit_on_texts(text_train) 
tokenizer.fit_on_texts(text_test)
 
train_seq = tokenizer.texts_to_sequences(text_train)
test_seq = tokenizer.texts_to_sequences(text_test)
 
padded_train = pad_sequences(train_seq)
padded_test = pad_sequences(test_seq)

import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

acc_thres = 0.99

class customCallback(tf.keras.callbacks.Callback): 
  def on_epoch_end(self, epoch, logs={}): 
    if(logs.get('accuracy') > acc_thres and logs.get('val_accuracy') > acc_thres):   
      print("\nReached %2.2f%% accuracy, stopping training . . ." %(acc_thres*100))   
      self.model.stop_training = True

callbacks = customCallback()

"""### train model

"""

history = model.fit(
    padded_train, 
    label_train, 
    epochs=2, 
    validation_data=(padded_test, label_test), 
    callbacks=[callbacks]
    )

import matplotlib.pyplot as plt

def plot_metric(history, metric):
  train_metrics = history.history[metric]
  val_metrics = history.history['val_' + metric]
  epochs = range(1, len(train_metrics) + 1)
  plt.rcParams["figure.figsize"] = (6,8)
  plt.plot(epochs, train_metrics)
  plt.plot(epochs, val_metrics)
  plt.title('Training and Validation ' + metric)
  plt.xlabel("Epochs")
  plt.ylabel(metric)
  plt.legend(["train_" + metric, 'val_' + metric], loc="best")
  plt.show()

plot_metric(history, 'loss')

plot_metric(history, 'accuracy')